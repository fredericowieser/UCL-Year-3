{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will explore various binary classification algortihms, including Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Logistic Regression, and Naive Bayes. \n",
    "\n",
    "We will apply these algorithms to one toy data set, and two real-world datasets: the Pima Indians Diabetes data set; and the Wisconsin Diagnostic Breast Cancer data set. \n",
    "\n",
    "We will evaluate performances using three metrics: Classifcation Accuracy, the Receiver Operating Characteristic, and Precision/Recall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "- The structure of the code is given to you and you will need to fill in the parts corresponding to each question. \n",
    "- Do not modify/erase other parts of the code if you have not been given specific instructions to do so.\n",
    "- When you are asked to insert code, do so between the areas which begin:\n",
    "  \n",
    "  `##########################################################`\n",
    "  \n",
    "  `# TO_DO`\n",
    "  \n",
    "  `# [your code here]`\n",
    "   \n",
    "   And which end:\n",
    "   \n",
    "  `# /TO_DO\n",
    "   ##########################################################`\n",
    "\n",
    "\n",
    "- When you are asked to comment on the results you should give clear and comprehensible explanations. Write the comments in a 'Code Cell' with a sign `#` at the beginning of each row, and in the areas which begin:\n",
    "\n",
    "  `# [INSERT YOUR ANSWER HERE]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please do not change the cell below, you will see a number of imports. All these packages are relevant for the assignment and it is important that you get used to them. You can find more information about them in their respective documentation. As usual Numpy, Pandas, and Scikit-Learn will be used heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from os import chdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# you may need to import sklearn.lda.LDA and sklearn.qda.QDA instead\n",
    "# depending on which version you have installed\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data Set\n",
    "\n",
    "### Generate Toy Data Set:\n",
    "\n",
    "We being by generating a toy dataset that contains two sets of Gaussian random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(18)\n",
    "\n",
    "n_sample, n_feature = 200, 2\n",
    "cov = np.array([[0., -0.2], [0.8, .2]])\n",
    "X = np.r_[np.dot(np.random.randn(n_sample, n_feature), cov),\n",
    "          np.dot(np.random.randn(n_sample, n_feature), cov) + np.array([1, 1])]\n",
    "\n",
    "y = np.hstack((np.zeros(n_sample), np.ones(n_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Decision Boundaries:\n",
    "\n",
    "Next we define a function to plot data points and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(X, y, method, model, idx_plot):\n",
    "    plt.subplot(1, 3, idx_plot)\n",
    "\n",
    "    X0, X1 = X[y == 0], X[y == 1]  # data corresponding to classes 0 and 1\n",
    "\n",
    "    plt.scatter(X0[:, 0], X0[:, 1], color='green')  # class 0\n",
    "    plt.scatter(X1[:, 0], X1[:, 1], color='blue')  # class 1\n",
    "\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, [0.5], colors='black')\n",
    "    \n",
    "    plt.title(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions:\n",
    "Finally we fit the data using Logistic Regression, LDA, and QDA models, and then visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver = 'lbfgs')\n",
    "lr.fit(X, y)\n",
    "plot_results(X, y, 'Logistic Regression', lr, 1)\n",
    "\n",
    "lda = LDA()\n",
    "lda.fit(X, y)\n",
    "plot_results(X, y, 'LDA', lda, 2)\n",
    "\n",
    "qda = QDA()\n",
    "qda.fit(X, y)\n",
    "plot_results(X, y, 'QDA', qda, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pima Indians Diabetes Dataset\n",
    "\n",
    "The Pima Diabetes Dataset is sampled from Pima people, a group of naive Americans that have developed the highest prevalence of type II diabetes. This is caused by a sudden change of their lifestyle, including diet and physical activities.\n",
    "\n",
    "The dataset contains data from 768 females with 8 features:\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "\n",
    "The last column indicates if the person is diagnosed with diabetes (value 1) or not (value 0).\n",
    "<br>\n",
    "<br>\n",
    "The original dataset is available at **[UCI Pima Indians Diabetes](http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set:\n",
    "\n",
    "The dataset is stored in _pima-indians-diabetes.csv_. Please make sure you have downloaded the file and changed the working directory to where you stored the file. You may want to check `os.chdir` to change directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following line changes your working directory to \n",
    "# your current working directory, you will need to modify the path below\n",
    "import os \n",
    "chdir(os.getcwd())\n",
    "\n",
    "# read .csv file with pandas.read_csv\n",
    "data = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# because the csv file does not contain header information, we manually add\n",
    "# headers of the dataset\n",
    "data.columns = [\"NumPreg\", \"PlGlcConc\", \"BloodP\",\"SkinThick\", \n",
    "                \"SerIns\", \"BMI\",\"DiPedFunc\", \"Age\", \"DiabetesStatus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Samples:\n",
    "Use `pandas.DataFrame.head()` to visualise the first several rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlations:\n",
    "\n",
    "Use `seaborn.heatmap` to plot pair-wise correlations between features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlations between features\n",
    "corr = data.corr()\n",
    "\n",
    "from seaborn import heatmap\n",
    "heatmap(corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Features - Histogram:\n",
    "\n",
    "__Task:__  \n",
    "Generate histograms of the first two features. (You may find `matplotlib.pyplot.histogram` useful). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can generate histograms with `pandas.DataFrame.hist`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Features - Box Plot:\n",
    "Yet another option is `pandas.DataFrame.plot()`. We can also use this to generate Box Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Features - Density Plot:\n",
    "And Density Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='density', subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(12, 9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing:\n",
    "\n",
    "In Week 1, we used `sklearn.preprocessing.Imputer` to clean the data. \n",
    "\n",
    "Another option is to use `pandas.DataFrame.replace`, if the type of your dataset is `pandas.DataFrame`.\n",
    "\n",
    "In this section, missing values of the following features are replaced with the mean value of the corresponding feature column. \n",
    "\n",
    "__Task:__  \n",
    "Clean other features if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Plasma glucose concentration a 2 hours in an oral glucose tolerance test (PlGlcConc)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_PlGlcConc = data['PlGlcConc'].mean()\n",
    "data['PlGlcConc'] = data['PlGlcConc'].replace(to_replace=0, value=mean_PlGlcConc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Diastolic blood pressure (BloodP)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_BloodP = data['BloodP'].mean()\n",
    "data['BloodP'] = data['BloodP'].replace(to_replace=0, value=mean_BloodP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Triceps skin fold thickness (SkinThick)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_SkinThick = data['SkinThick'].mean()\n",
    "data['SkinThick'] = data['SkinThick'].replace(to_replace=0, value=mean_SkinThick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Other Features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa9ab15a62bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmean_BMI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BMI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BMI'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BMI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_BMI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data Set:\n",
    "\n",
    "Before we start, we need to split the data set into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels from the dataset\n",
    "X, y = data.drop(\"DiabetesStatus\", axis=1), data[\"DiabetesStatus\"].copy()\n",
    "\n",
    "# split the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data Set:\n",
    "Next we standardise the data set since the features are reported on different scales. \n",
    "\n",
    "Common scaling techniques include normalisation and binarisation. \n",
    "\n",
    "`sklearn.preprocessing` provides various feature scaling modules (use `help(preprocessing)` to check the documentation of these modules). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver = 'lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "y_lr = lr.predict(X_test)\n",
    "y_lr_prob = lr.predict_proba(X_test)[:, -1]  # probability estimates of the positive class\n",
    "y_lr_score = lr.decision_function(X_test)  # confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "y_lda = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "qda.fit(X_train, y_train)\n",
    "y_qda = qda.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performances:\n",
    "Finally we compare the performances with three different metrics: Classification Accuracy, ROC curve, and Precision-Recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Classification Accuracy:_\n",
    "Classification accuracy is perhaps the most popular and (apparently) natural metric for evaluating the performance of a classification algorithm. \n",
    "\n",
    "It can be conveniently computed by using `sklearn.metrics.accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary variable with keys being algorithms names and values being classification predictions\n",
    "results = dict(zip(['logistic regression', 'naive bayes', 'LDA', \n",
    "               'QDA'],\n",
    "              [y_lr, y_nb, y_lda, y_qda]))\n",
    "\n",
    "# create a dictionary variable with keys being algorithm names and values being classification accuracy\n",
    "accuracy = {}\n",
    "for algorithm, prediction in results.items():\n",
    "    accuracy[algorithm] = accuracy_score(y_test, prediction)\n",
    "    \n",
    "accuracy = pd.DataFrame(data=accuracy, index=[0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _ROC Curve:_\n",
    "A Receiver Operating Characteristic curve (ROC curve) is a graphical plot of the true positive rate (TPR) against the false positive rate (FPR), using different threshold settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_lr_prob)  # false positive rates and true positive rates\n",
    "\n",
    "# plot the ROC curve\n",
    "fig, axis = plt.subplots()\n",
    "axis.plot(fpr, tpr)\n",
    "axis.plot([0, 1], [0, 1], 'k--')\n",
    "axis.set_xlabel('false positive rate')\n",
    "axis.set_ylabel('true positive rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Precision/Recall:_\n",
    "\n",
    "The Precision/Recall, can be computed as follows:\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Here *TP* is the number of true positives, *FP* is the number of false positives and *FN* is the number of false negatives. \n",
    "\n",
    "The terms 'positive' and 'negative' in the classification context refer to the prediction, and the terms 'true' and 'false' refer to whether the prediction corresponds to the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_lr_score)  # precision, recall scores\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.plot(thresholds, precision[:-1], label='precision')  # plot precision\n",
    "axis.plot(thresholds, recall[:-1], label='recall')  # plot recall\n",
    "axis.set_xlabel('threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisconsin Diagnostic Breast Cancer Data Set\n",
    "\n",
    "We have applied several binary classification algorithms to the Pima Indians Diabetes dataset. \n",
    "\n",
    "Now we will try a simialr exercise using the Wisconsin Diagnostic Breast Cancer data set (https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).\n",
    "\n",
    "The features from the Breast Cancer data set are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  \n",
    "The data set contains data from 569 females, whose diagosis of breast tissues are either malignant (212 samples) or benign (357 samples).  \n",
    "The Breast Cancer data set has been pre-processed and cleaned with Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set:\n",
    "The dataset can be loaded with `sklearn.datasets.load_breast_cancer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer['data']\n",
    "y = breast_cancer['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptions of the columns of the data set can be viewed with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(breast_cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data Set:\n",
    "__Task:__  \n",
    "Begin by splitting the data set into train and test parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression:\n",
    "__Task:__  \n",
    "Next apply the Logistic Regression algorithm to train a model, then apply it to the test data and save the resulting predictions to a variable `y_lr`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes:\n",
    "__Task:__  \n",
    "Next apply the Naive Bayes algorithm to train a model, then apply it to the test data and save the resulting predictions to a variable `y_nb`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA:\n",
    "__Task:__  \n",
    "Next apply the Naive Bayes algorithm to train a model, then apply it to the test data and save the resulting predictions to a variable `y_lda`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA:\n",
    "__Task:__  \n",
    "Next apply the Naive Bayes algorithm to train a model, then apply it to the test data and save the resulting predictions to a variable `y_qda`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performances:\n",
    "\n",
    "__Task:__  \n",
    "Finally, compare performances of your classification algorithms and visualise your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# TO_DO\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "##########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
