{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will look at Simple Linear Regression, Multiple Linear Regression and Polynomial Regression as part of which we will touch upon the concepts of Under/Overfitting and Regularisation. For the most part we will either use the Normal Equations in our analysis. However the normal equations become unwieldy with large amounts of data, so at the end we will touch upon numerical optimisation and Gradient Descent in particular.\n",
    "\n",
    "On the coding side we will use Python, Numpy, Pandas and Scikit-Learn. And we will code up some of the simpler algorithms and then compare the results with those of Scikit-Learn's implementation.\n",
    "\n",
    "There are many advanced forms of regression that we won't touch upon including those using Kernels, Gaussian Processes, Neural Nets, Decision Trees, Boostong etc. However, the idea here is to give a thorough feel for key concepts, and to introduce you to some useful tools for manipulating them.\n",
    "\n",
    "Some of the code in this notebook originally comes from code accompanying 'Hands-On Machine Learning with Scikit-Learn & Tensorflow', by A. Geron, and some from this tutorial on Ridge Regression and the Lasso in Python:\n",
    "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "- The structure of the code is given to you and you will need to fill in the parts corresponding to each question. \n",
    "- Do not modify/erase other parts of the code if you have not been given specific instructions to do so.\n",
    "- When you are asked to insert code, do so between the areas which begin:\n",
    "  \n",
    "  `##########################################################`\n",
    "  \n",
    "  `# TO_DO`\n",
    "  \n",
    "  `# [your code here]`\n",
    "   \n",
    "   And which end:\n",
    "   \n",
    "  `# /TO_DO\n",
    "   ##########################################################`\n",
    "\n",
    "\n",
    "- When you are asked to comment on the results you should give clear and comprehensible explanations. Write the comments in a 'Code Cell' with a sign `#` at the beginning of each row, and in the areas which begin:\n",
    "\n",
    "  `# [INSERT YOUR ANSWER HERE]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please do not change the cell below, you will see a number of imports. All these packages are relevant for the assignment and it is important that you get used to them. You can find more information about them in their respective documentation. As usual Numpy, Pandas, and Scikit-Learn will be used heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Python libraries for data and visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Import error metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Import a dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#Import data munging tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Display charts in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data\n",
    "\n",
    "We will begin by creating some linear data with Gaussian noise added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create some one dimensional data with a bit of noise\n",
    "num_points = 50\n",
    "X = np.linspace(0,100,num_points).reshape(num_points,1)\n",
    "y = (4 + 3 * X) + 25*np.random.randn(num_points, 1)\n",
    "\n",
    "#Plot the data\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.title('Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "The function below accepts training data in the form of a design matrix, $\\mathbf{X}$ and target data in the form of an output vector, $\\mathbf{y}$ and returns optimal parameters, $\\mathbf{w}$. \n",
    "As in the lectures, $\\mathbf{w}$ characterises the weight vector associated with some linear mapping, $f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x}$, which seeks to map inputs to outputs.\n",
    "\n",
    "Notice that in this case we have 50 one-dimensional training points, so $\\mathbf{X}$ will be a numpy array of shape (50,1).  \n",
    "And we have one input attribute, and one bias term, so in this case $\\mathbf{w}$ will be a Numpy array of shape (2,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn $\\mathbf{w}$ using the Normal Equations:\n",
    "\n",
    "__Task:__  \n",
    "Now, write a function to fit the data and return optimal weight parameters:\n",
    "\n",
    "_Steps:_  \n",
    "a) Prepend a vector of ones onto the training data (to include the bias term)  \n",
    "b) Calculate `w_best` using the Normal Equations.\n",
    "\n",
    "(Try to make the function as general as possible by allowing it to accept higher dimensional inputs in order to save further work when you do Multiple Linear Regression later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_regression(X, y):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "    \n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Now, write a function that, given parameters `w` and input test data `X_test`, returns predictions `y_pred`.\n",
    "\n",
    "`X_test` will be of dimension (50,1), and in the case of  Simple Linear Regression, `w` will be of dimension (2,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, X_test):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "    \n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Use your two functions to create predictions given the input data `X`.  \n",
    "Call the variable containing the predictions `ypred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn $\\mathbf{w}$ using Scikit-Learn:\n",
    "\n",
    "__Task:__  \n",
    "1) Create a linear regression model using Scikit-Learn  \n",
    "2) Fit the model using `X` and `y`  \n",
    "3) Predict `y` given the fitted model and `X`. Put this predicition in a variable `y_lr`  \n",
    "4) Calculate Mean Squared Errors for the hand coded and the Scikit-Learn models. Put the results in variables `mse` and `mse_lr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison:\n",
    "\n",
    "If everything has been carried out correctly then when you run the code in the next cell you should see the training data (as blue dots), the line of best fit from your own code, `ypred`, and the best fit from your Scikit-Learn implementation (green '+'s).  \n",
    "The red line should be a good fit to the blue training data and the green '+'s should be an exact match to the red line.  \n",
    "The mean squared errors are also reported and should be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THIS CELL\n",
    "# We wish to plot the variables you created: ypred, y_lr\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, ypred, 'r-')\n",
    "plt.plot(X, y_lr, 'g+')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()\n",
    "print(\"MSE - Sci kit\",mse_lr,\",\",\"MSE - Hand-Coded\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "We will now examine Multiple Linear Regression, and this time we will go directly through the Scikit-Learn workflow.\n",
    "\n",
    "To avoid data cleaning/ munging etc. we will use the Boston housing data from the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PLEASE DO NOT CHANGE THIS CELL: Load the Boston data again\n",
    "boston = load_boston()\n",
    "X_boston = boston['data']\n",
    "y_boston = boston['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing & Model Training:\n",
    "\n",
    "__Task:__  \n",
    "1) Create a train/test split in the Boston data set.  \n",
    "(_Hint:_ Use `train_test_split()`, choose `test_size=0.2`, and `random_state=42`, and call the variables containing the train(test) inputs `X_train`(`X_test`) and the train(test) targets `y_train`(`y_test`).  \n",
    "2) Linear regression parameter outputs are dependent on the scale of the features. Use `StandardScalar` to fit/transform the training data and to fit the test data. (Consider why we do not also fit on the test data.)  \n",
    "3) Create a linear regression model, fit it, and store predictions for `X_test` in a variable called `y_lr_boston`.  \n",
    "4) How good is your trained model on the test data? Calculate the Mean Squared Error between `y_test` and `y_lr_boston`, and store the result in a variable called `mse_lr_boston`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "# Split\n",
    "\n",
    "# Standardise\n",
    "\n",
    "# Fit\n",
    "\n",
    "# MSE\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Now use your own model as before.\n",
    "\n",
    "(If you had already coded your function to handle multiple dimensions then you have no alterations to make to the Simple Linear Regression function, otherwise you will need to generalise a little for higher dimensional data).\n",
    "\n",
    "_Steps:_  \n",
    "1) Use the Simple Linear Regression function with training inputs from your standard scalar to fit optimal weight parameters `w_boston`  \n",
    "2) Make your predictions given your parameters and input test data. Store your predictions for `X_test` in a variable called `ypred_boston`  \n",
    "3) Calculate the mean squared errors of your predictions. Store your answer in a variable called `mse_ypred_boston`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison:\n",
    "\n",
    "Because we are dealing with multiple dimensions, visualisation is less simple. \n",
    "\n",
    "Below we show our model compared with the Scikit-Learn version: we create a scatter graph against of the `y_test` results. The pattern should be close to identical. \n",
    "\n",
    "We also show the Mean Squared Errors on the test set of the two models: These should be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True)\n",
    "axes[0].scatter(ypred_boston,y_test)\n",
    "axes[0].set_title(\"Linear Regression\")\n",
    "axes[1].scatter(y_lr_boston,y_test)\n",
    "axes[1].set_title(\"SKLearn Linear Regression\")\n",
    "\n",
    "\n",
    "print(\"MSE-hand coded\",mse_ypred_boston, \"MSE-Sci-Kit\",mse_lr_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Regression\n",
    "\n",
    "We will now examine Non-Linear Regression.  \n",
    "In particular we will consider Polynomial Regression with an underlying cubic function. Again noise is added.  \n",
    "In terms of training our model will only see the noisy data. Meanwhile we will use the underlying cubic as our 'ground truth' or  test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "num_points = 50\n",
    "X = np.linspace(-3,5,num_points).reshape(num_points,1) \n",
    "y_clean = X**3 \n",
    "y = y_clean + 25*np.random.randn(num_points, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Scale the training data, `X`, using the `StandardScalar`. Store the result in `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the scaled training data, noting that the $x_1$ axis should have been rescaled with axis extremes around -1.5/1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "#Plot the data\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features:\n",
    "\n",
    "We are still performing an underlying linear regression, but this time we transform the input variables. We can use Scikit-Learn to affect theis transformation for us using `PolynomialFeatures`.\n",
    "\n",
    "__Task:__  \n",
    "1) Create polynomial features using `PolyFeatures`. Be sure to set the value of degree in the loop.  \n",
    "2) `fit_transform` the values `X`  \n",
    "3) Create a `LinearRegression` model and fit the transformed features on the training targets `y`.  \n",
    "4) Create predictions for `X_poly` using your trained model. Store the results in a variable called `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "mses_train = []\n",
    "mses_test = []\n",
    "degrees = range(1,30)\n",
    "for degree in degrees:\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "    # Create polynomial features\n",
    "    \n",
    "    # Transform X\n",
    "    \n",
    "    # Create a linear regression model and fit\n",
    "    \n",
    "    # Create predictions y_pred\n",
    "    \n",
    "    # /TO_DO\n",
    "    ########################################################    \n",
    "    \n",
    "    #Please do not change code below\n",
    "    y_preds.append(y_pred)\n",
    "    mses_train.append(mean_squared_error(y, y_pred))\n",
    "    mses_test.append(mean_squared_error(y_clean, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Results:\n",
    "\n",
    "This is a rather noisy graph: The training data is depicted with blue dots, and we hope to learn the ground truth, represented by the green solid line.\n",
    "\n",
    "The various fits plotted include a range from linear regression to a 29-dimensional polynomial regression. \n",
    "\n",
    "If the implementation is correct you should see various lines fitting the training data points as the degree becomes higher.\n",
    "\n",
    "__Task:__  \n",
    "Consider which polynomial is the best fit and why?  \n",
    "How would you decide given only training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "plt.plot(X, y, 'b.', label='Training Data')\n",
    "plt.plot(X, y_clean, 'g', label='Ground Truth')\n",
    "plt.plot(X, y_preds[0], 'c--', label='1D Poly')\n",
    "plt.plot(X, y_preds[2], 'r--', label='3D Poly')\n",
    "plt.plot(X, y_preds[15], 'y--',label='16D Poly')\n",
    "plt.plot(X, y_preds[28], 'm--',label='29D Poly')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Errors:\n",
    "\n",
    "The chart below plots the Mean Squared Error of your results.\n",
    "\n",
    "As the degree of your polynomial increases, the training error decreases. \n",
    "\n",
    "On the other hand this behaviour is not manifest for the unknown test set: Below a certain complexity we have a large bias; Above a certain complexity we begin to have a model that not only fits the underlying process but also the noise. \n",
    "\n",
    "This is the Bias / Variance Trade-Off. In order to fit the best model to the data we must employ model selection techniques, for example cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plt.plot(degrees,mses_train,label=\"Train\")\n",
    "plt.plot(degrees,mses_test,label=\"Test\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.legend()\n",
    "plt.title(\"Train and Test Mean Squared Errors for polynomials\")\n",
    "print('On the test set the lowest MSE comes from a polynomial of degree:',np.argmin(np.array(mses_test))+1)\n",
    "print('Min test MSE: ',np.min(np.array(mses_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Regression by Hand:\n",
    "\n",
    "In the cell below we show the training data, `X`, the training targets, `y`, and the features we would use for Polynomial Regression. \n",
    "\n",
    "It is simply a Linear Regression with the input features transformed. \n",
    "\n",
    "We create these in order to hand-code a facsimile of the Scikit-Learn implementation which we encountered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "data = pd.DataFrame(np.column_stack([X,y]),columns=['X','y'])\n",
    "\n",
    "for i in range(2,30):  \n",
    "    colname = 'X_%d'%i      \n",
    "    data[colname] = data['X']**i\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we present a function that performs Linear Regression and plots and calculates the residual sum of squares on a number of polynomial models.\n",
    "\n",
    "__Task:__  \n",
    "1) What is the Residual Sum of Squares?  \n",
    "2) How is it different to the Mean Squared Error and why do we care about all these error functions?  \n",
    "3) If we performed classification would we have a different error function for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    predictors=['X']\n",
    "    if power>=2:\n",
    "        predictors.extend(['X_%d'%i for i in range(2,power+1)])\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression() #normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['X'],y_pred)\n",
    "        plt.plot(data['X'],data['y'],'.')\n",
    "        #plt.plot(data['X'],y_clean)\n",
    "        plt.title('Plot for power: %d'%power)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([linreg.intercept_])\n",
    "    ret.extend(linreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we show the fit for a number of polynomials of varying degrees. \n",
    "\n",
    "__Task:__  \n",
    "Which do you think show bias? And which over-fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_X_%d'%i for i in range(1,30)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,30)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the powers for which a plot is required:\n",
    "models_to_plot = {1:231,3:232,6:233,12:234,18:235,29:236}\n",
    "\n",
    "#Iterate through all powers and assimilate results\n",
    "for i in range(1,30):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient Matrix:\n",
    "\n",
    "The cell below shows the Residual Sum of Squares and coefficients for each of the regressions. For example the cubic has an intercept and three coeffients. \n",
    "\n",
    "__Task:__  \n",
    "1) What do you notice about the Residual Sum of Squares as the degree of the polynomial increases?\n",
    "2) What do you notice about the size of the coefficents as the degree of the polynomial increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "In the previous cells we fitted a cubic with noise added, using various polynomials. Low degree polynomials lacked predictive power and were biased, however as the polynomial degree increased the model started overfitting to the noise. \n",
    "\n",
    "There are various methods of dealing with this. \n",
    "\n",
    "In the cell above we observed that the absolute value of the coefficients for the parameters increases as the complexity of our model increases. \n",
    "\n",
    "Regularisation methods place a penalty on the norm of these coefficients in our loss function.\n",
    "\n",
    "__Task:__  \n",
    "Look up and understand the difference between $\\ell_2$ and $\\ell_1$ regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_2$ Regularisation / Ridge Regression:\n",
    "\n",
    "In the cell below we use Ridge Regression using Scikit-Learn's `Ridge` model. Notice the `alpha` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha) #,normalize=True)\n",
    "    ridgereg.fit(data[predictors],data['y'])\n",
    "    y_pred = ridgereg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['X'],y_pred)\n",
    "        plt.plot(data['X'],data['y'],'.')\n",
    "        #plt.plot(data['X'],y_clean)\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([ridgereg.intercept_])\n",
    "    ret.extend(ridgereg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of `alpha` in Ridge Regression:\n",
    "\n",
    "In the cell below we perform a regression with a 29-degree polynomial. Normally this would overfit this data.  \n",
    "The charts show a Ridge Regression with varying `alpha` parameter settings.\n",
    "\n",
    "__Task:__  \n",
    "1) How would one tune `alpha` in practise to achieve good generalisation?  \n",
    "2) What do you notice about the charts as `alpha` changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "predictors=['X']\n",
    "predictors.extend(['X_%d'%i for i in range(2,30)])\n",
    "\n",
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-20, 1e-15, 1e-8, 1e-4, 1e-3,1e-2, 0.1, 5, 20, 1000]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss','intercept'] + ['coef_X_%d'%i for i in range(1,30)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-20:231, 1e-8:232, 1e-3:233, 0.1:234, 5:235, 1000:236}\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_2$ Regularised Coefficient Matrix:\n",
    "\n",
    "For the 29th degree polynomial we display the coefficients and Residual Sum of Squares for various `alpha` settings.  \n",
    "For comparison the unregularised version would be similar to the top line of the table.\n",
    "\n",
    "__Task:__  \n",
    "1) What do you notice about the Residual Sum of Squares as `alpha` increases?  \n",
    "2) Do any of the coefficients go to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_1$ Regularisation / the LASSO:\n",
    "\n",
    "In the cells below we perform the same regressions but this time using $\\ell_1$ regularisation.\n",
    "\n",
    "__Task:__  \n",
    "1) What do you notice about the charts?  \n",
    "2) Is there a difference to Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['X'],y_pred)\n",
    "        plt.plot(data['X'],data['y'],'.')\n",
    "        #plt.plot(data['X'],y_clean)\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "predictors=['X']\n",
    "predictors.extend(['X_%d'%i for i in range(2,30)])\n",
    "\n",
    "#Define the alpha values to test\n",
    "alpha_lasso = [1e-15, 1e-3, 1e-2, 0.1,2, 1,2,3, 4, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,30)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-15:231, 1e-3:232,1e-2:233, 0.1:234, 1:235, 10:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_1$ Regularised Coefficient Matrix:\n",
    "\n",
    "Again we display the Residual Sum of Squares and coefficients for the 29th degree polynomial for varying `alpha` settings.\n",
    "\n",
    "__Tasks:__  \n",
    "1) In a least squares sense which will have a lower error on the training data: unregularised or $\\ell_2$ regularised regression?  \n",
    "2) In a least squares sense which will have a lower error on the training data: $\\ell_2$ or $\\ell_1$ regularised regression?  \n",
    "3) What do you notice about the coefficients for $\\ell_1$ regularisation as `alpha` increases?  \n",
    "4) Is there any particular behaviour that stands out as `alpha` increases which was not manifest in the $\\ell_2$ case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "coef_matrix_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Gradient Descent\n",
    "\n",
    "Recall the Normal Equations:\n",
    "\\begin{align}\n",
    "\\mathbf{w} = (\\mathbf{X}^{T} \\mathbf{X})^{-1}\\mathbf{X}\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "These equations become unwieldy as data scales.\n",
    "\n",
    "__Task:__  \n",
    "Why is this?\n",
    "\n",
    "Numerical optimisation techniques may be used as an alternative. One such technique is Gradient Descent.   \n",
    "In the example below we wish to fit the data using Batch Gradient Descent. (To keep things straightforward we shall go back to our original linear data plus noise example). \n",
    "\n",
    "__Task:__  \n",
    "Research the differences between Batch, Mini-Batch and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "num_points = 50\n",
    "X = np.linspace(0,100,num_points).reshape(num_points,1)\n",
    "y = (4 + 3 * X) + 25*np.random.randn(num_points, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#Plot the data, we want to divine a best fit\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.title('Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Batch Gradient Descent:\n",
    "\n",
    "__Task:__  \n",
    "1) Derive the gradient of the error with respect to the weight parameters in linear regression.\n",
    "2) Implement gradient linear regression in code.\n",
    "\n",
    "_Hints:_  \n",
    "a) First add a bias as you did previously.  \n",
    "b) Create a loop ranging to the number of iterations we wish to perform (`n_iterations`).  \n",
    "c) Calculate the gradients within the loop . \n",
    "d) Update the variable `w_best` by taking a step-size `eta` in the opposite direction to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_linear_regression(X, y, n_iterations):\n",
    "    eta = 0.01\n",
    "    n_iterations = n_iterations \n",
    "    m = X.shape[0]\n",
    "    w_best = np.random.randn(X.shape[1]+1,1)\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "    \n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "theta_best_10 = gradient_linear_regression(X, y,10)\n",
    "theta_best_30 = gradient_linear_regression(X, y,30)\n",
    "theta_best_50 = gradient_linear_regression(X, y,30)\n",
    "theta_best_100 = gradient_linear_regression(X, y,100)\n",
    "theta_best_500 = gradient_linear_regression(X, y,500)\n",
    "\n",
    "ypred_10 = pred(theta_best_10,X)\n",
    "ypred_30 = pred(theta_best_30,X)\n",
    "ypred_50 = pred(theta_best_50,X)\n",
    "ypred_100 = pred(theta_best_100,X)\n",
    "ypred_500 = pred(theta_best_500,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Results:\n",
    "\n",
    "The training data is fitted by Linear Regression with Gradient Descent, however we have used a different number of epochs. (Epochs are the number of iterations we perform before we stop learning).\n",
    "\n",
    "__Task:__  \n",
    "1) What do you notice as the epochs increase?  \n",
    "2) How would you decide the number of epochs to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plt.plot(X, y, 'b.', label='Training Data')\n",
    "plt.plot(X, ypred_10, label='Prediction 10 epochs')\n",
    "plt.plot(X, ypred_30, label='Prediction 30 epochs')\n",
    "plt.plot(X, ypred_50, label='Prediction 50 epochs')\n",
    "plt.plot(X, ypred_100,label='Prediction 100 epochs')\n",
    "plt.plot(X, ypred_500, label='Prediction 500 epochs')\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "\n",
    "plt.title(\"Linear Regression using Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
