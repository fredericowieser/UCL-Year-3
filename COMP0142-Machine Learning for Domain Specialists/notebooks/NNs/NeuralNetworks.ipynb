{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will code a simple Neural Network from scratch.   \n",
    "To truly understand Neural Nets it is useful to hand code one at least once and derive the equations. \n",
    "\n",
    "We will use the MNIST data set, a standard testbed for simple machine learning tasks, which is described in the wiki: https://en.wikipedia.org/wiki/MNIST_database. It comprises a training set of 60,000 handwritten images, and a test set of 10,000. Each data point consists of a 24x24 pixel image, which represents one of 10 handwritten numbers. It is often used for classification purposes, however we will apply different dimensionality and clustering methods to the data set.\n",
    "\n",
    "A good book on neural nets is by Ian Goodfellow: http://www.deeplearningbook.org/\n",
    "\n",
    "A nice online book giving many intuitions and deriving backprop is found in: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "Documentation on Keras may be found at: https://keras.io/\n",
    "\n",
    "Finally, a visual beginner's guide to CNN's may be found at: https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "- The structure of the code is given to you and you will need to fill in the parts corresponding to each question. \n",
    "- Do not modify/erase other parts of the code if you have not been given specific instructions to do so.\n",
    "- When you are asked to insert code, do so between the areas which begin:\n",
    "  \n",
    "  `##########################################################`\n",
    "  \n",
    "  `# TO_DO`\n",
    "  \n",
    "  `# [your code here]`\n",
    "   \n",
    "   And which end:\n",
    "   \n",
    "  `# /TO_DO\n",
    "   ##########################################################`\n",
    "\n",
    "\n",
    "- When you are asked to comment on the results you should give clear and comprehensible explanations. Write the comments in a 'Code Cell' with a sign `#` at the beginning of each row, and in the areas which begin:\n",
    "\n",
    "  `# [INSERT YOUR ANSWER HERE]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please do not change the cell below, you will see a number of imports. All these packages are relevant for the assignment and it is important that you get used to them. You can find more information about them in their respective documentation. As usual Numpy, Pandas, and Scikit-Learn will be used heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We will begin by loading the MNIST data set and setting a few variables such as the number of epochs which we wish to train our Neural Network for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "num_epochs =100\n",
    "num_classes = 10\n",
    "batch_size= 128\n",
    "learn_rate = 0.05\n",
    "conv_epochs = 12\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "tr_loss, ts_loss, tr_acc, ts_acc = [], [], [], []\n",
    "\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "trainX = trainX.astype('float32')\n",
    "testX =testX.astype('float32')\n",
    "trainX /= 255\n",
    "testX /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the Data\n",
    "\n",
    "The cell below shows a random training image, and illustrates the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "def show_random_image():\n",
    "    i = random.randint(0,trainX.shape[0])\n",
    "    input_image = trainX[i]\n",
    "    plt.imshow(input_image)\n",
    "    \n",
    "show_random_image()\n",
    "\n",
    "print(\"Training input shape: \", trainX.shape)\n",
    "print(\"Training target shape: \", trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "1) The target data `trainY` and `testY` should be one-hot-encoded. There are 10 classes.  \n",
    "2) We wish to reshape the inputs (`trainX` and `testX`) to the Neural Network to be of shape (number of images, 784). For example, currently `trainX` is of shape (60000,28,28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Activation Functions and Their Derivatives\n",
    "\n",
    "The expressiveness of a Neural Net comes from applying non-linear activation functions. There may be linear layers, however a function composed solely of linear layers remains linear. Why?\n",
    "\n",
    "__Task:__      \n",
    "Fill in the code to write the activation functions which we will use, as well as their derivatives.  \n",
    "Remember that our inputs will be numpy arrays, not scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################    \n",
    "\n",
    "    \n",
    "def sigmoid_prime(x):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################    \n",
    "\n",
    "    \n",
    "def relu(x):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################    \n",
    "\n",
    "    \n",
    "def relu_prime(x):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################    \n",
    "\n",
    "    \n",
    "def softmax(x):\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cost Function\n",
    "\n",
    "__Question:__   \n",
    "In the cell below we have created a cost function.   \n",
    "There are various cost functions that could be used. Which cost function is this one, and why is it suitable as an error for MNIST classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "def cost(y_truth, y_pred):\n",
    "    return -((y_truth * np.log(y_pred)).sum())/y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Weights and Store Activations and their Derivatives\n",
    "\n",
    "__Questions:__   \n",
    "1) Why is the input weight of dimension 784?   \n",
    "2) Why is the output weight of dimension 10?   \n",
    "3) How many parameters will the Neural Net have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "weights = [np.random.randn(*w) * 0.1 for w in [(784, 100), (100, 50), (50,10)]]\n",
    "biases = [np.random.randn(*b) * 0.1 for b in [(100,), (50,), (10,)]]\n",
    "activations = [(sigmoid, sigmoid_prime), (relu, relu_prime), (softmax, None)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Function to Perform Forward Propogation\n",
    "\n",
    "We wish to be able to input batches of training images and output predictions.   \n",
    "\n",
    "__Task:__  \n",
    "In the function below return a list, `a`, containing the numpy arrays which comprise the outputs from each layer of the Neural Net.   \n",
    "The final layer will comprise our predictions.\n",
    "\n",
    "To be clear, the return value from forward() should have the following shape:\n",
    "\n",
    "Forward prop on one image: \n",
    "\n",
    "Layer  0  shape = (1, 784)\n",
    "\n",
    "Layer  1  shape = (1, 100)\n",
    "\n",
    "Layer  2  shape = (1, 50)\n",
    "\n",
    "Layer  3  shape = (1, 10)\n",
    "\n",
    "Forward prop on batch of 10 images: \n",
    "\n",
    "Layer  0  shape = (10, 784)\n",
    "\n",
    "Layer  1  shape = (10, 100)\n",
    "\n",
    "Layer  2  shape = (10, 50)\n",
    "\n",
    "Layer  3  shape = (10, 10)\n",
    "\n",
    "To be clear, the batch size doesn't change the dimensionality of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights, biases, act):\n",
    "    a = [X]\n",
    "    \n",
    "    for w,b,f in zip(weights, biases, act):\n",
    "        ########################################################\n",
    "        # TO_DO\n",
    "        #[your code here]\n",
    "\n",
    "        \n",
    "        # /TO_DO\n",
    "        ########################################################\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Dimensionality of the Return Value for the Forward Propogation Function\n",
    "\n",
    "This should be as given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print(\"Our list of activations has 4 elements, each a numpy array\")\n",
    "print(\"Forward prop on first image: How does dimensionality work?\")\n",
    "a_test1 = forward(trainX[0].reshape(-1,784),weights, biases,activations)\n",
    "for i in range(len(a_test1)):\n",
    "    print(\"Layer \",i, \" shape =\",a_test1[i].shape)\n",
    " \n",
    "print(' ')\n",
    "print(\"Forward prop on batch of 10 images: How does dimensionality work?\")  \n",
    "a_test10 = forward(trainX[0:10].reshape(-1,784),weights, biases,activations)\n",
    "for i in range(len(a_test10)):\n",
    "    print(\"Layer \",i, \" shape =\",a_test10[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Gradients of our Error with respect to all the Neural Network Parameters\n",
    "\n",
    "Here we will use backpropogation.\n",
    "\n",
    "__Task:__   \n",
    "Follow the instructions in the code segment below, and complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, Y, weights, biases, act):\n",
    "    # Perform a forward pass and store the return values in a variable named a:\n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "    \n",
    "    # /TO_DO\n",
    "    ########################################################\n",
    "\n",
    "    #Inialise gradients of weights and biases\n",
    "    gradients = np.empty_like(weights)\n",
    "    grad_bias = np.empty_like(biases)\n",
    "\n",
    "    #Here we perform back propogation on the output layer only\n",
    "    #Be sure to understand how this works by examining the differential of the loss function\n",
    "    delta = a[-1] - Y\n",
    "    gradients[-1] = a[-2].T.dot(delta)\n",
    "    grad_bias[-1] = np.sum(delta, axis=0)\n",
    "\n",
    "    for i in range(len(a)-2,0,-1):\n",
    "        #For each layer calculate the gradients\n",
    "        #For example:\n",
    "        #grad_bias[i-1] = ?\n",
    "        #gradients[i-1] = ?\n",
    "        ########################################################\n",
    "        # TO_DO\n",
    "        #[your code here]\n",
    "        \n",
    "    \n",
    "        # /TO_DO\n",
    "        ########################################################\n",
    "\n",
    "    #Divide weight gradients and biases by the batch size....why?\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "\n",
    "\n",
    "    # /TO_DO\n",
    "    ########################################################\n",
    "\n",
    "    return gradients, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense Check\n",
    "\n",
    "The code below calls your function for one image and checks the sizes of your layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print(\"Gradients and biases - : How does dimensionality work?\")\n",
    "a = forward(trainX[0].reshape(-1,784),weights, biases,activations)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    grad_test1, grad_bias1 = gradients(trainX[0].reshape(-1,784), trainY[0], weights, biases, activations)\n",
    "    print(\"Layer :\", i, \" Gradients, Biases - shape =\", grad_test1[i].shape, grad_bias1[i].shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gradients and Biases should have the Following Dimensionality:\n",
    "\n",
    "Gradients and biases - : \n",
    "    \n",
    "Layer : 0  Gradients, Biases - shape = (784, 100) (100,)\n",
    "    \n",
    "Layer : 1  Gradients, Biases - shape = (100, 50) (50,)\n",
    "    \n",
    "Layer : 2  Gradients, Biases - shape = (50, 10) (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "\n",
    "The outer loop will be the number of epochs we wish to perform, for each epoch we will loop in batches, thus this is mini-batch gradient descent.\n",
    "\n",
    "__Task:__   \n",
    "1) Within the inner loop calculate the gradients and biases and store in variables `gradw`, `biases`.   \n",
    "2) Within the inner loop update the `weights` and `biases` using the respective gradients, and set the step size via the variable `learn_rate` defined earlier.   \n",
    "3) In the outer loop create predictions on the training and test set, then store the results in variables `trpreds`, `tstpreds`.    \n",
    "4) Next calculate the training and test cost, and store the results in variables `ctr`, `cst`.\n",
    "\n",
    "We will choose the argmax of those predictions and use this to calculate our the accuracy of our predictions.\n",
    "\n",
    "\n",
    "__Task:__     \n",
    "Understand the code and printed outputs.    \n",
    "How well would you expect the final model to perform on unseen MNIST images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_epochs):\n",
    "    for j in range(0, len(trainX), batch_size):\n",
    "        X, Y = trainX[j:j+batch_size], trainY[j:j+batch_size]\n",
    "\n",
    "        ########################################################\n",
    "        # TO_DO\n",
    "        #[your code here]\n",
    "\n",
    "        #gradw, grad_bias = ?\n",
    "        #biases = ?\n",
    "        #weights = ?\n",
    "                \n",
    "        # /TO_DO\n",
    "        ########################################################\n",
    "\n",
    "        \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "    #[your code here]\n",
    "    \n",
    "    #trpreds = ?\n",
    "    #tstpreds = ?\n",
    "    #ctr = ?\n",
    "    #cst = ?\n",
    "    \n",
    "    # /TO_DO\n",
    "    ########################################################\n",
    "    \n",
    "    #we will wrap up the rest...\n",
    "    tr_loss.append(ctr)\n",
    "    ts_loss.append(cst)\n",
    "    \n",
    "    trpred_onehot = np.argmax(trpreds, axis=1)\n",
    "    tstpred_onehot = np.argmax(tstpreds, axis=1)\n",
    "    \n",
    "    train_acc = np.mean(trpred_onehot == np.argmax(trainY, axis=1))\n",
    "    test_acc = np.mean(tstpred_onehot == np.argmax(testY, axis=1))\n",
    "    tr_acc.append(train_acc)\n",
    "    ts_acc.append(test_acc)\n",
    "    \n",
    "    if i%5==0:\n",
    "        print(\"Epoch:\", i, \"Tr_loss:\", ctr, \"Tst_loss:\", cst , \"Tr_acc:\",train_acc, \"Tst_acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Graph Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plt.plot(tr_loss, label='Manual Training Loss')\n",
    "plt.plot(ts_loss, label='Manual Test Loss')\n",
    "\n",
    "#plt.plot(history.history['loss'], label='Keras Training Loss')\n",
    "#plt.plot(history.history['val_loss'], label='Keras Test Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Hand Coded versus Adam Losses')\n",
    "plt.ylabel('Cross Entropy loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_acc, label='Manual Train Acc')\n",
    "plt.plot(ts_acc, label='Manual Test Acc')\n",
    "\n",
    "#plt.plot(history.history['acc'], label='Keras Train Acc')\n",
    "#plt.plot(history.history['val_acc'], label='Keras Test Acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Hand Coded versus Adam Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Predictions\n",
    "\n",
    "Let's take a look at the predictions for the first 10 images in the test set.\n",
    "\n",
    "__Task:__   \n",
    "1) Check the predictions are mostly correct (this particular model has about 96% accuracy).   \n",
    "2) Change the code to find images where the prediction is incorrect. Are these hard even for a human to classify or is the Neural Net making obvious errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "for i in range(10):\n",
    "    input_image = testX[i].reshape(1,784)\n",
    "    \n",
    "    tstpreds = forward(input_image, weights, biases, activations)[-1]\n",
    "    \n",
    "    fig            = plt.figure(i, figsize=(15, 11))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    plt.imshow(input_image.reshape(28,28))\n",
    "    plt.xlabel(\"Neural Net prediction: \" + str(np.argmax(tstpreds)))#, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Activations\n",
    "\n",
    "Here we take the first 9 test images and show the input image together with the activations for each hidden layer. \n",
    "\n",
    "This is really for interest only and not easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "for i in range(9):\n",
    "\n",
    "    input_image = testX[i].reshape(1,784)\n",
    "    tstpreds = forward(input_image, weights, biases, activations)\n",
    "\n",
    "    fig = plt.figure( figsize=(12, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(tstpreds[0].reshape(28,28))\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(tstpreds[1].reshape(10,10))\n",
    "    plt.title(\"Hidden Layer 1 activations\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(tstpreds[2].reshape(5,10))\n",
    "    plt.title(\"Hidden Layer 2 activations\")\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Softmax Outputs\n",
    "\n",
    "Again for the first 9 images we now examine the softmax outputs for the Neural Net.\n",
    "\n",
    "The title shows the unseen test label and there should be high probabilities of a correct prediction for the trained model.  \n",
    "\n",
    "However examine the final histogram.   \n",
    "This has an incorrect prediction, and notice that the net was far less confident about its prediction and there were a few other competing choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "for i in range(9):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    input_image = testX[i].reshape(1,784)\n",
    "    tstpreds = forward(input_image, weights, biases, activations)\n",
    "\n",
    "    plt.bar(np.arange(10),list(tstpreds[-1][0])) #pred[-1]))\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(\"Test number \"+ str(np.argmax(testY[i]))+\": Output Layer probabilities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
